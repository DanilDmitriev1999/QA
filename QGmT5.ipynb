{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QGmT5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "927f38939f074f489b9d2e0ddba3534b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c70e3cdd582f4a8aba457e748ca338fb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_165e590a061f4ef188b1607d3bd78389",
              "IPY_MODEL_87e43876f35b4c5986cf51e571003c22"
            ]
          }
        },
        "c70e3cdd582f4a8aba457e748ca338fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "165e590a061f4ef188b1607d3bd78389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f176a84f4bd54738888621c95b6fbbc1",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e369fcd7541b4cd1ae31c6f8e192d404"
          }
        },
        "87e43876f35b4c5986cf51e571003c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a0f9bf6ced6d49e29404c08ca5f4caa2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:01&lt;00:00,  1.31it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_719f097b3d7f4c619a6d20d2e7dc523d"
          }
        },
        "f176a84f4bd54738888621c95b6fbbc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e369fcd7541b4cd1ae31c6f8e192d404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0f9bf6ced6d49e29404c08ca5f4caa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "719f097b3d7f4c619a6d20d2e7dc523d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc8cf667749e4970a1dce09f61f5c0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f9d84192a68845cb88a084ca1fd0288f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e8f3970e425f452d9396265dfdf283ee",
              "IPY_MODEL_40848a9972a0493cbfe6e77ae26bd9ac"
            ]
          }
        },
        "f9d84192a68845cb88a084ca1fd0288f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "e8f3970e425f452d9396265dfdf283ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b38e193d6b5641f3b5209bf7974d6f0f",
            "_dom_classes": [],
            "description": "Epoch 0:  44%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 11962,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5311,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2acbc29ca84a4e5a9d34c586f39b7ec3"
          }
        },
        "40848a9972a0493cbfe6e77ae26bd9ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1a68c56ae2c545c0a6004d57e8677722",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5311/11962 [43:28&lt;54:26,  2.04it/s, loss=3.27, v_num=0]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8265038e6d64b1c95b34f73a6a3ca8f"
          }
        },
        "b38e193d6b5641f3b5209bf7974d6f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2acbc29ca84a4e5a9d34c586f39b7ec3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a68c56ae2c545c0a6004d57e8677722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8265038e6d64b1c95b34f73a6a3ca8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHgFmQTn0R4m"
      },
      "source": [
        "# Техническая часть"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_HdEsGI0Lxf",
        "outputId": "46278538-518a-4657-deb5-2c002df62a41"
      },
      "source": [
        "!pip install -q sentencepiece\n",
        "import sentencepiece\n",
        "!pip install -q transformers\n",
        "\n",
        "!pip install -q comet_ml\n",
        "import comet_ml\n",
        "!pip install -q pytorch-lightning\n",
        "\n",
        "!pip -q install datasets\n",
        "!pip -q install rouge_score\n",
        "\n",
        "!git clone -q https://github.com/DanilDmitriev1999/QA"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.2MB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 38.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 52.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 522kB 14.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25h  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 808kB 10.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 21.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 276kB 52.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 33.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 829kB 53.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 50.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 54.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 53.6MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 225kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 40.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2oZItwx0VCi",
        "outputId": "3449905b-3ee9-4cf3-e232-567ac695a6be"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import comet_ml\n",
        "\n",
        "import numpy as np\n",
        "import collections\n",
        "import functools\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "\n",
        "from io import open\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "from typing import List\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AdamW, MT5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup)\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import CometLogger\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "\n",
        "from QA.DataModule.dataset import *\n",
        "from QA.DataModule.reader import *\n",
        "\n",
        "from QA.model.BERT import *\n",
        "from QA.utils.trainer import *\n",
        "\n",
        "seed_everything(294)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device == 'cuda':\n",
        "    from torch.cuda import LongTensor\n",
        "else:\n",
        "    from torch import LongTensor\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 294\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqkzHdR50tHA"
      },
      "source": [
        "# Данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP12xtUA0rZW"
      },
      "source": [
        "train_file_path = '/content/QA/data/sber_squad/train-v1.1.json'\n",
        "dev_file_path = '/content/QA/data/sber_squad/dev-v1.1.json'\n",
        "train = ReadData(train_file_path)\n",
        "train_data = train.data\n",
        "dev = ReadData(dev_file_path)\n",
        "dev_data = dev.data\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/mt5-small')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy5oAR7w1HlR",
        "outputId": "2580ca67-a013-481e-f194-6f4a0dbe17c5"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': 'В допетровское время искусство в России имело своим призванием служить исключительно религиозным целям, а так как православная церковь гнушается изваяниями человеческих фигур, то скульптура, в настоящем смысле слова, не могла в древней Руси не только развиваться, но и существовать. Правда, в некоторых местах, в особенности в бывших новгородских областях, пользовались уважением резные и раскрашенные изображения святых, но они были чужды всякого художественного значения и составляли изделия, возникшие под влиянием Запада. Собственно же на Руси проявления пластики ограничивались литьем небольших крестов, образов-складней, выбиванием окладов на образа и резьбою фигурных иконостасов. В числе плодов западно-европейской цивилизации Пётр Великий перенес в него и скульптуру, которая, однако, при этом государе и долго после него находилось здесь в руках приезжих иностранцев. Главным деятелем по части скульптуры в царствование Петра Великого и Анны Иоанновны был К. Б. Растрелли, отец знаменитого впоследствии архитектора, вызванный в Петербург для литья пушек. Об его манерном стиле свидетельствуют бронзовая статуя императрицы Анны, и монумент Петру Великому, стоящий перед Инженерным замком в Санкт-Петербурге.',\n",
              " 'id': '4095',\n",
              " 'qas': [{'answers': [{'answer_start': 583,\n",
              "     'text': 'литьем небольших крестов, образов-складней, выбиванием окладов на образа и резьбою фигурных иконостасов.'}],\n",
              "   'id': '50619',\n",
              "   'question': 'Чем на Руси ограничивались проявления пластики?'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9aA_ThAuLaV",
        "outputId": "65c753cd-17cc-410a-c466-8400cdeb5320"
      },
      "source": [
        "tokenizer.special_tokens_map"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eos_token': '</s>', 'pad_token': '<pad>', 'unk_token': '<unk>'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJNASZDTxJBF",
        "outputId": "cc71f700-0d41-4cc1-a323-a55907328a06"
      },
      "source": [
        "tokenizer('<s>')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [1042, 263, 669, 1], 'attention_mask': [1, 1, 1, 1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3lRpMYd1-qw"
      },
      "source": [
        "class QGDataset:\n",
        "    def __init__(self, dataset: List[dict], tokenizer) -> None:\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer_context = lambda x: tokenizer.encode_plus(x,\n",
        "                                                         add_special_tokens=True,\n",
        "                                                         padding='max_length',\n",
        "                                                         max_length=350,\n",
        "                                                         truncation=True,\n",
        "                                                         return_tensors=\"pt\")\n",
        "        self.tokenizer_question = lambda x: tokenizer.encode_plus(x,\n",
        "                                                         add_special_tokens=True,\n",
        "                                                         padding='max_length',\n",
        "                                                         max_length=200,\n",
        "                                                         truncation=True,\n",
        "                                                         return_tensors=\"pt\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx) -> dict:\n",
        "        context = self.dataset[idx]['context'][0:150]\n",
        "        question = self.dataset[idx]['qas'][0]['question'][0:100]\n",
        "        encodings_context = self.tokenizer_context(context)\n",
        "        encoding_question = self.tokenizer_question(question)\n",
        "\n",
        "        result = {\n",
        "            'input_ids': encodings_context['input_ids'].flatten(),\n",
        "            'target_ids': encoding_question['input_ids'].flatten(),\n",
        "            'input_attention_mask': encodings_context['attention_mask'].flatten(),\n",
        "            'target_attention_mask': encoding_question['attention_mask'].flatten(),\n",
        "        }\n",
        "        # print(result)\n",
        "        return result\n",
        "\n",
        "        "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm-2RAeL4neH"
      },
      "source": [
        "def collate_fn(examples):\n",
        "    return tokenizer.pad(examples, return_tensors='pt')\n",
        "\n",
        "train_dataset = QGDataset(train_data, tokenizer)\n",
        "train_iter = DataLoader(dataset=QGDataset(train_data, tokenizer),\n",
        "                        batch_size=4)\n",
        "dev_iter = DataLoader(dataset=QGDataset(dev_data, tokenizer),\n",
        "                        batch_size=8)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuV0QDgjR733"
      },
      "source": [
        "for _ in train_iter:\n",
        "    pass"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4knqEXiJ4s-M"
      },
      "source": [
        "next(iter(train_iter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EB3f_QMFGvS"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI-zz5F64vZ-"
      },
      "source": [
        "import time\n",
        "\n",
        "\n",
        "class QGmT5model(pl.LightningModule):\n",
        "    def __init__(self, lr):\n",
        "        super().__init__()\n",
        "        self.lr = lr\n",
        "        self.model = MT5ForConditionalGeneration.from_pretrained('google/mt5-small')\n",
        "\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained('google/mt5-small')\n",
        "        self.rouge = load_metric('rouge')\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def parse_score(self, result):\n",
        "        return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n",
        "    \n",
        "    def forward(self, input_ids, input_attention_mask, \n",
        "                target_attention_mask=None, target_ids=None,):\n",
        "        \n",
        "        result = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=input_attention_mask,\n",
        "            labels=target_ids)\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def lmap(self, f, x):\n",
        "        return list(map(f, x))\n",
        "    \n",
        "\n",
        "    def _step(self, batch):\n",
        "        lm_labels = batch['target_ids']\n",
        "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        # print(f'input_ids: {batch[\"input_ids\"].shape}')\n",
        "        # print(f'input_attention_mask: {batch[\"input_attention_mask\"].shape}')\n",
        "\n",
        "        # print(f'target_idx: {batch[\"target_ids\"].shape}')\n",
        "        # print(f'target_attention_mask: {batch[\"target_attention_mask\"].shape}')\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            input_attention_mask=batch[\"input_attention_mask\"],\n",
        "            target_ids=batch[\"target_ids\"],\n",
        "            target_attention_mask=batch['target_attention_mask']\n",
        "        )\n",
        "    \n",
        "        loss = outputs[0]\n",
        "        return loss\n",
        "    \n",
        "    def ids_to_clean_text(self, generated_ids):\n",
        "        gen_text = self.tokenizer.batch_decode(\n",
        "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        return self.lmap(str.strip, gen_text)\n",
        "    \n",
        "    def _generative_step(self, batch) :\n",
        "        \n",
        "        t0 = time.time()\n",
        "        \n",
        "        generated_ids = self.model.generate(\n",
        "            batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"input_attention_mask\"],\n",
        "            use_cache=True,\n",
        "            decoder_attention_mask=batch['target_attention_mask'],\n",
        "            max_length=100, \n",
        "            num_beams=2,\n",
        "            repetition_penalty=2.5, \n",
        "            length_penalty=1.0, \n",
        "            early_stopping=True\n",
        "        )\n",
        "        preds = self.ids_to_clean_text(generated_ids)\n",
        "        target = self.ids_to_clean_text(batch[\"target_ids\"])\n",
        "            \n",
        "        gen_time = (time.time() - t0) / batch[\"input_ids\"].shape[0]  \n",
        "    \n",
        "        loss = self._step(batch)\n",
        "        base_metrics = {'val_loss': loss}\n",
        "        self.log_dict({'val_loss': loss})\n",
        "\n",
        "        summ_len = np.mean(self.lmap(len, generated_ids))\n",
        "        base_metrics.update(gen_time=gen_time, gen_len=summ_len, preds=preds, target=target)\n",
        "        print(preds)\n",
        "        print(target)\n",
        "        self.rouge.add_batch(predictions=preds, references=target)\n",
        "        \n",
        "#         rouge_results = self.rouge_metric.compute() \n",
        "#         rouge_dict = self.parse_score(rouge_results)\n",
        "#         base_metrics.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n",
        "        \n",
        "        return base_metrics\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "\n",
        "        logs = {\"train_loss\": loss}\n",
        "\n",
        "        self.log_dict(logs)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._generative_step(batch)\n",
        "    \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        \n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        logs = {\"val_loss\": avg_loss}\n",
        "        \n",
        "        rouge_results = self.rouge.compute() \n",
        "        rouge_dict = self.parse_score(rouge_results)\n",
        "    \n",
        "        logs.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n",
        "\n",
        "        self.log_dict(logs)\n",
        "        \n",
        "        ## Clear out the lists for next epoch\n",
        "        self.target_gen= []\n",
        "        self.prediction_gen=[]\n",
        "        return {\"avg_val_loss\": avg_loss, \n",
        "                \"rouge1\" : rouge_results['rouge1'],\n",
        "                \"rougeL\" : rouge_results['rougeL']}\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.model.parameters(), lr=self.lr, eps=1e-8)\n",
        "        return optimizer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0BqOvUv2p07",
        "outputId": "c774004e-e9f4-4289-faed-005b803fced5"
      },
      "source": [
        "comet_logger = CometLogger(\n",
        "    api_key=\"HWfJT3eyByVJWe4nEbi1pGosA\",\n",
        "    workspace=\"danildmitriev1999\",\n",
        "    project_name=\"qa\",\n",
        "    experiment_name=\"mT5 QG\",\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CometLogger will be initialized in online mode\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk20fuoxP-MQ",
        "outputId": "f12f5574-5433-439e-f17c-5f574d9b8dcc"
      },
      "source": [
        "lr = 3e-4\n",
        "\n",
        "N_EPOCHS = 1\n",
        "CLIP = 1.5\n",
        "\n",
        "model = QGmT5model(lr).to(device)\n",
        "\n",
        "trainer = Trainer(max_epochs=N_EPOCHS,\n",
        "                  gpus=1,\n",
        "                gradient_clip_val=CLIP,\n",
        "                progress_bar_refresh_rate=1,\n",
        "                log_every_n_steps=3,\n",
        "                )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGFU1wtNRWEn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324,
          "referenced_widgets": [
            "927f38939f074f489b9d2e0ddba3534b",
            "c70e3cdd582f4a8aba457e748ca338fb",
            "165e590a061f4ef188b1607d3bd78389",
            "87e43876f35b4c5986cf51e571003c22",
            "f176a84f4bd54738888621c95b6fbbc1",
            "e369fcd7541b4cd1ae31c6f8e192d404",
            "a0f9bf6ced6d49e29404c08ca5f4caa2",
            "719f097b3d7f4c619a6d20d2e7dc523d",
            "bc8cf667749e4970a1dce09f61f5c0fc",
            "f9d84192a68845cb88a084ca1fd0288f",
            "e8f3970e425f452d9396265dfdf283ee",
            "40848a9972a0493cbfe6e77ae26bd9ac",
            "b38e193d6b5641f3b5209bf7974d6f0f",
            "2acbc29ca84a4e5a9d34c586f39b7ec3",
            "1a68c56ae2c545c0a6004d57e8677722",
            "d8265038e6d64b1c95b34f73a6a3ca8f"
          ]
        },
        "outputId": "a2f2904f-506a-4b24-d466-3658afe0da63"
      },
      "source": [
        "trainer.fit(model, train_iter, dev_iter)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                        | Params\n",
            "------------------------------------------------------\n",
            "0 | model | MT5ForConditionalGeneration | 300 M \n",
            "------------------------------------------------------\n",
            "300 M     Trainable params\n",
            "0         Non-trainable params\n",
            "300 M     Total params\n",
            "1,200.707 Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "927f38939f074f489b9d2e0ddba3534b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['<extra_id_0>. <extra_id_10>', '<extra_id_0>.', '<extra_id_0>.', '<extra_id_0>.) <extra_id_37> с двигателем внутреннего сгорания', '<extra_id_0>.', '<extra_id_0>. <extra_id_10>. <extra_id_10>', '<extra_id_0>.', '<extra_id_0> Швейцарии']\n",
            "['Чем заболел Байрон в Миссолонги?', 'Как отводятся излишки тепла у млекопитающих?', 'Что нарушают хромосомные аберрации?', 'В каком городе первый в мире городской автобус с двигателем внутреннего сгорания вышел на маршрут?', 'В каком режиме проходят подлимитные операции?', 'Находит ли отражение в алфавите фонетическая связь слогов?', 'Каким стал стиль последних работ художника?', 'Сколько человек родилось в 2008 году у граждан Швейцарии?']\n",
            "['<extra_id_0>.', '<extra_id_0>.', '<extra_id_0>.', '<extra_id_0>.', '<extra_id_0>.', '<extra_id_0>.', '<extra_id_0>.', '<extra_id_0>.']\n",
            "['Какая статья и пункт дает возможность статья для легализации ПО, скачанного из Интернет и предоставл', 'Какими русскими купцами был накоплен значимый торговый опыт?', 'Где была применена конкурентная разведка через несколько лет?', 'Совместимы ли операционные системы POSIX?', 'Сколько раз Реджинальд Дохерти стал чемпионом с 1897 по 1900 год?', 'Кто считал что русская публицистическая литература восходит к „Слову о Законе и Благодати“ Илариона?', 'Где колючие кустарники и засухоустойчивые злаки формируют земную кору?', 'Какое важное событие произошло с Мадонной в 14 лет?']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 294\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc8cf667749e4970a1dce09f61f5c0fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goBIKnfvTuhi"
      },
      "source": [
        "# from comet_ml import Experiment\n",
        "# name = 't5_QG'\n",
        "# trainer.save_checkpoint(f\"/content/save_models/{name}.ckpt\")\n",
        "# experiment = Experiment(\n",
        "#     api_key='HWfJT3eyByVJWe4nEbi1pGosA', project_name='qa',workspace='danildmitriev1999')\n",
        "\n",
        "# experiment.log_model(\"t5_QG\", f\"/content/save_models/{name}.ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCnsuKn2Sd8f"
      },
      "source": [
        "def generate_question(context, tokenizer, model):\n",
        "    tokens = tokenizer.encode_plus(context,\n",
        "                                    add_special_tokens=True,\n",
        "                                    padding='max_length',\n",
        "                                    max_length=350,\n",
        "                                    truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "                tokens[\"input_ids\"],\n",
        "                attention_mask=tokens[\"attention_mask\"],\n",
        "                use_cache=True,\n",
        "                max_length=100, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "            )\n",
        "    \n",
        "    pred = [tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) \n",
        "    for generated_id in generated_ids]\n",
        "\n",
        "    return pred"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46gv5xZxeyzQ"
      },
      "source": [
        "tr_model = model.model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARubS4l3e_JJ",
        "outputId": "e18789a5-d329-4f97-f51d-fc6be843efdd"
      },
      "source": [
        "dev_data[8]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': 'Иногда говорят о возможности провести GNU GPL как договор присоединения, согласно статьям (428, 435 ГК РФ). Но единственный такой способ для лицензионных договоров описан в п. 3 ст. 1286 ГК РФ ( Заключение лицензионных договоров о предоставлении права использования программы для ЭВМ или базы данных допускается путём заключения каждым пользователем с соответствующим правообладателем договора присоединения, условия которого изложены на приобретаемом экземпляре таких программы или базы данных либо на упаковке этого экземпляра, а также в электронном виде (пункт 2 статьи 434). ). Эта статья даёт возможность для легализации ПО, скачанного из Интернет и предоставляемого по лицензии GNU GPL (Лицензионный договор, заключаемый в упрощённом порядке, является безвозмездным, если договором не предусмотрено иное.).',\n",
              " 'id': '13772',\n",
              " 'qas': [{'answers': [{'answer_start': 558, 'text': 'пункт 2 статьи 434'}],\n",
              "   'id': '60110',\n",
              "   'question': 'Какая статья и пункт дает возможность статья для легализации ПО, скачанного из Интернет и предоставляемого по лицензии GNU GPL?'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ykUHGWZe8aV",
        "outputId": "411d1784-9abb-41ce-c357-a1f5eb412467"
      },
      "source": [
        "generate_question(dev_data[8]['context'], tokenizer, tr_model)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Что является основой лицензии GNU GPL?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2_RfIwhfJ-d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}